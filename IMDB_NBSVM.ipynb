{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "IMDB_NBSVM.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "Kca0ZSj7mOas",
        "colab_type": "code",
        "outputId": "5f868377-e6d2-4ce7-ff14-3f50520bdfa0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 275
        }
      },
      "source": [
        "!pip install PyDrive\n",
        "\n",
        "from pydrive.auth import GoogleAuth\n",
        "from pydrive.drive import GoogleDrive\n",
        "from google.colab import auth\n",
        "from oauth2client.client import GoogleCredentials\n",
        "\n",
        "# Authenticate and create the PyDrive client.\n",
        "# This only needs to be done once in a notebook.\n",
        "auth.authenticate_user()\n",
        "gauth = GoogleAuth()\n",
        "gauth.credentials = GoogleCredentials.get_application_default()\n",
        "drive = GoogleDrive(gauth)\n",
        "\n",
        "fileId = drive.CreateFile({'id': '#'}) \n",
        "print(fileId['title'])  # dataset.zip\n",
        "fileId.GetContentFile('temp.zip')  # Save Drive file as a local file\n",
        "\n",
        "!unzip temp.zip -d ./"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: PyDrive in /usr/local/lib/python3.6/dist-packages (1.3.1)\n",
            "Requirement already satisfied: google-api-python-client>=1.2 in /usr/local/lib/python3.6/dist-packages (from PyDrive) (1.6.7)\n",
            "Requirement already satisfied: PyYAML>=3.0 in /usr/local/lib/python3.6/dist-packages (from PyDrive) (3.13)\n",
            "Requirement already satisfied: oauth2client>=4.0.0 in /usr/local/lib/python3.6/dist-packages (from PyDrive) (4.1.3)\n",
            "Requirement already satisfied: httplib2<1dev,>=0.9.2 in /usr/local/lib/python3.6/dist-packages (from google-api-python-client>=1.2->PyDrive) (0.11.3)\n",
            "Requirement already satisfied: uritemplate<4dev,>=3.0.0 in /usr/local/lib/python3.6/dist-packages (from google-api-python-client>=1.2->PyDrive) (3.0.0)\n",
            "Requirement already satisfied: six<2dev,>=1.6.1 in /usr/local/lib/python3.6/dist-packages (from google-api-python-client>=1.2->PyDrive) (1.12.0)\n",
            "Requirement already satisfied: rsa>=3.1.4 in /usr/local/lib/python3.6/dist-packages (from oauth2client>=4.0.0->PyDrive) (4.0)\n",
            "Requirement already satisfied: pyasn1-modules>=0.0.5 in /usr/local/lib/python3.6/dist-packages (from oauth2client>=4.0.0->PyDrive) (0.2.5)\n",
            "Requirement already satisfied: pyasn1>=0.1.7 in /usr/local/lib/python3.6/dist-packages (from oauth2client>=4.0.0->PyDrive) (0.4.5)\n",
            "imdb_master.zip\n",
            "Archive:  temp.zip\n",
            "replace ./imdb_master.csv? [y]es, [n]o, [A]ll, [N]one, [r]ename: y\n",
            "  inflating: ./imdb_master.csv       \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h64nXIYRmVK5",
        "colab_type": "code",
        "outputId": "5f780ae3-7f3e-420e-c991-0dd84eb9d4c0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        }
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk.stem.porter import PorterStemmer\n",
        "import re\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn import metrics\n",
        "from sklearn.model_selection import train_test_split\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "from keras.layers.core import Activation\n",
        "from keras.models import Model\n",
        "from keras.layers import Input, Embedding, Flatten, dot\n",
        "from keras import backend as K\n",
        "from keras.optimizers import Adam"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QUF4DjYJmbHl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df = pd.read_csv('imdb_master.csv', encoding = \"ISO-8859-1\")\n",
        "del df['Unnamed: 0']\n",
        "del df['file']\n",
        "del df['type']"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bHdIN8_hmxlf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df = df.loc[0:49999]\n",
        "df = df.sample(frac=1).reset_index(drop=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zv-mi8u5mzkq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def clean_reviews(text):\n",
        "    lemmatizer = WordNetLemmatizer()\n",
        "    my_stopwords = stopwords.words('english') \n",
        "    text = text.replace(\"<br >\", \"\")\n",
        "    text = text.replace(\"</br >\", \"\")        \n",
        "    text = re.sub('[^a-zA-Z]',' ', text)\n",
        "    text = text.lower() \n",
        "    text = [lemmatizer.lemmatize(token) for token in text.split(\" \")]\n",
        "    text = [lemmatizer.lemmatize(token, \"v\") for token in text]\n",
        "    text = [word for word in text if not word in my_stopwords]\n",
        "    text = \" \".join(text)\n",
        "    return text  "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hLcaOdZwm2C9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df[\"review\"]=df.review.apply(lambda x: clean_reviews(x))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eSyxX0GPm4F_",
        "colab_type": "code",
        "outputId": "349e8726-90f7-4361-baab-1d8b1408966e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        }
      },
      "source": [
        "df[\"label\"] = df[\"label\"].map({'pos': 1, 'neg': 0})\n",
        "df.head()"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>review</th>\n",
              "      <th>label</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>pure crap  probably worst biblical theme film ...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>north africa       small arab town edge sahara...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>even  theatre person   highly recommend see  b...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>familiar story man  writer  sell soul devil or...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>go bother plot synopsis since know movie almos...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                              review  label\n",
              "0  pure crap  probably worst biblical theme film ...      0\n",
              "1  north africa       small arab town edge sahara...      1\n",
              "2  even  theatre person   highly recommend see  b...      1\n",
              "3  familiar story man  writer  sell soul devil or...      0\n",
              "4  go bother plot synopsis since know movie almos...      0"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Kn85MAWano8c",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(df[\"review\"], df[\"label\"], test_size=0.2)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-ndw5JjgpulX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "veczr =  CountVectorizer(ngram_range=(1,3), binary=True, token_pattern=r'\\w+', max_features=800000)\n",
        "dtm_train = veczr.fit_transform(X_train)\n",
        "dtm_test = veczr.transform(X_test)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QfqY882HNgon",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "a3b87235-d93a-41b9-a9a4-c51caf139207"
      },
      "source": [
        "def dtm2wid(dtm, maxlen=2000):\n",
        "    x = []\n",
        "    nwds = []\n",
        "    for idx, row in enumerate(dtm):\n",
        "        seq = []\n",
        "        indices = (row.indices + 1).astype(np.int64)\n",
        "        np.append(nwds, len(indices))\n",
        "        data = (row.data).astype(np.int64)\n",
        "        count_dict = dict(zip(indices, data))\n",
        "        for k,v in count_dict.items():\n",
        "            seq.extend([k]*v)\n",
        "        num_words = len(seq)\n",
        "        nwds.append(num_words)\n",
        "        # pad up to maxlen\n",
        "        if num_words < maxlen: \n",
        "            seq = np.pad(seq, (maxlen - num_words, 0), mode='constant')\n",
        "        # truncate down to maxlen\n",
        "        else:                  \n",
        "            seq = seq[-maxlen:]\n",
        "        x.append(seq)\n",
        "    nwds = np.array(nwds)\n",
        "    print('sequence stats: avg:%s, max:%s, min:%s' % (nwds.mean(), nwds.max(), nwds.min()) )\n",
        "    return np.array(x)\n",
        "\n",
        "maxlen = 2000\n",
        "x_train = dtm2wid(dtm_train, maxlen=maxlen)\n",
        "x_test = dtm2wid(dtm_test, maxlen=maxlen)"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "sequence stats: avg:192.97095, max:2146, min:5\n",
            "sequence stats: avg:172.109, max:1224, min:12\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Oy1Xx854UpF0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def pr(dtm, y, y_i):\n",
        "    p = dtm[y==y_i].sum(0)\n",
        "    return (p+1) / ((y==y_i).sum()+1)\n",
        "\n",
        "y_train = np.array(y_train) \n",
        "\n",
        "a = pr(dtm_train, y_train, 1)\n",
        "nbratios = np.log(pr(dtm_train, y_train, 1)/pr(dtm_train, y_train, 0))\n",
        "nbratios = np.squeeze(np.asarray(nbratios))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xWWzdV_0WtkR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_model(num_words, maxlen, nbratios=None):\n",
        "    embedding_matrix = np.zeros((num_words, 1))\n",
        "    for i in range(1, num_words): # skip 0, the padding value\n",
        "        if nbratios is not None:\n",
        "            # if log-count ratios are supplied, then it's NBSVM\n",
        "            embedding_matrix[i] = nbratios[i-1]\n",
        "        else:\n",
        "            # if log-count rations are not supplied, this reduces to a logistic regression\n",
        "            embedding_matrix[i] = 1\n",
        "\n",
        "    # set up the model\n",
        "    inp = Input(shape=(maxlen,))\n",
        "    r = Embedding(num_words, 1, input_length=maxlen, weights=[embedding_matrix], trainable=False)(inp)\n",
        "    x = Embedding(num_words, 1, input_length=maxlen, embeddings_initializer='glorot_normal')(inp)\n",
        "    x = dot([r,x], axes=1)\n",
        "    x = Flatten()(x)\n",
        "    x = Activation('sigmoid')(x)\n",
        "    model = Model(inputs=inp, outputs=x)\n",
        "    model.compile(loss='binary_crossentropy',\n",
        "                  optimizer=Adam(lr=0.001),\n",
        "                  metrics=['accuracy'])\n",
        "    return model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2YtscjWl7Mhr",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 391
        },
        "outputId": "738da245-5f7e-456d-e240-f87ade21c0d1"
      },
      "source": [
        "num_words = len([v for k,v in veczr.vocabulary_.items()]) + 1\n",
        "model = get_model(num_words, maxlen, nbratios=nbratios)\n",
        "model.fit(x_train, y_train,\n",
        "          batch_size=32,\n",
        "          epochs=10,\n",
        "          validation_data=(x_test, y_test))"
      ],
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 40000 samples, validate on 10000 samples\n",
            "Epoch 1/10\n",
            "40000/40000 [==============================] - 25s 617us/step - loss: 0.3098 - acc: 0.9230 - val_loss: 0.2663 - val_acc: 0.9109\n",
            "Epoch 2/10\n",
            "40000/40000 [==============================] - 24s 607us/step - loss: 0.1214 - acc: 0.9777 - val_loss: 0.2376 - val_acc: 0.9140\n",
            "Epoch 3/10\n",
            "40000/40000 [==============================] - 24s 605us/step - loss: 0.0692 - acc: 0.9918 - val_loss: 0.2262 - val_acc: 0.9148\n",
            "Epoch 4/10\n",
            "40000/40000 [==============================] - 24s 601us/step - loss: 0.0431 - acc: 0.9963 - val_loss: 0.2208 - val_acc: 0.9161\n",
            "Epoch 5/10\n",
            "40000/40000 [==============================] - 24s 604us/step - loss: 0.0278 - acc: 0.9986 - val_loss: 0.2188 - val_acc: 0.9151\n",
            "Epoch 6/10\n",
            "40000/40000 [==============================] - 24s 602us/step - loss: 0.0185 - acc: 0.9993 - val_loss: 0.2189 - val_acc: 0.9154\n",
            "Epoch 7/10\n",
            "40000/40000 [==============================] - 24s 603us/step - loss: 0.0125 - acc: 0.9995 - val_loss: 0.2208 - val_acc: 0.9136\n",
            "Epoch 8/10\n",
            "40000/40000 [==============================] - 24s 606us/step - loss: 0.0086 - acc: 0.9997 - val_loss: 0.2236 - val_acc: 0.9123\n",
            "Epoch 9/10\n",
            "40000/40000 [==============================] - 24s 604us/step - loss: 0.0059 - acc: 0.9999 - val_loss: 0.2279 - val_acc: 0.9117\n",
            "Epoch 10/10\n",
            "40000/40000 [==============================] - 24s 604us/step - loss: 0.0042 - acc: 1.0000 - val_loss: 0.2323 - val_acc: 0.9114\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7ff30276f9e8>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9ORig-Ya8aWD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Thanks to this blog : https://medium.com/@asmaiya/a-neural-implementation-of-nbsvm-in-keras-d4ef8c96cb7c"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5rG4RfUuJMAb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}
